{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887b7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8597392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_preprocessing.csv')\n",
    "test_df = pd.read_csv('test_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8e6931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>7604</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WorldNews Fallen powerlines on Glink tram UPDA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>7605</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>on the flip side Im at Walmart and there is a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>7606</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>7608</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>7612</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>The Latest More Homes Razed by Northern Califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 keyword location  \\\n",
       "0              0    None     None   \n",
       "1              1    None     None   \n",
       "2              2    None     None   \n",
       "3              3    None     None   \n",
       "4              4    None     None   \n",
       "...          ...     ...      ...   \n",
       "7498        7604    None     None   \n",
       "7499        7605    None     None   \n",
       "7500        7606    None     None   \n",
       "7501        7608    None     None   \n",
       "7502        7612    None     None   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this earthquake Ma...       1  \n",
       "1                 Forest fire near La Ronge Sask Canada       1  \n",
       "2     All residents asked to shelter in place are be...       1  \n",
       "3     13000 people receive wildfires evacuation orde...       1  \n",
       "4     Just got sent this photo from Ruby Alaska as s...       1  \n",
       "...                                                 ...     ...  \n",
       "7498  WorldNews Fallen powerlines on Glink tram UPDA...       1  \n",
       "7499  on the flip side Im at Walmart and there is a ...       1  \n",
       "7500  Suicide bomber kills 15 in Saudi security site...       1  \n",
       "7501  Two giant cranes holding a bridge collapse int...       1  \n",
       "7502  The Latest More Homes Razed by Northern Califo...       1  \n",
       "\n",
       "[7503 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f68d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df['text']\n",
    "y = train_df['target']\n",
    "x1 = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e554954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(x.values, y.values, test_size=.20, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3613056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 15:44:47.540848: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-27 15:44:47.542266: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-27 15:44:47.571289: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-27 15:44:47.572020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-27 15:44:48.140517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from transformers import TFRobertaForSequenceClassification\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c875dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo mô hình\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode train input\n",
    "encoded_inputs = tokenizer(train_texts.tolist(), padding=True, truncation=True, return_tensors='tf')\n",
    "labels = tf.constant(train_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb3458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fccfb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "188/188 [==============================] - 930s 5s/step - loss: 0.4975 - accuracy: 0.7642\n",
      "Epoch 2/2\n",
      "188/188 [==============================] - 893s 5s/step - loss: 0.3833 - accuracy: 0.8426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa03c5e60a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.fit(encoded_inputs.input_ids, labels, epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e446a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1501,), dtype=int32, numpy=array([0, 1, 0, ..., 1, 1, 1], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a10f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 41s 837ms/step - loss: 0.3968 - accuracy: 0.8434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#Evaluate the model\\npredictions = model.predict(test_inputs.input_ids)\\npredicted_labels = eval_predictions = tf.argmax(tf.nn.softmax(predictions, axis=1), axis=1).numpy()\\n\\naccuracy = accuracy_score(test_labels, predicted_labels)\\nprint(\"Accuracy:\", accuracy)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encode test input\n",
    "test_inputs = tokenizer(test_texts.tolist(), padding=True, truncation=True, return_tensors='tf')\n",
    "#labels = tf.constant(test_labels.tolist())\n",
    "\n",
    "\n",
    "model.evaluate(\n",
    "    x=test_inputs.input_ids,\n",
    "    y=test_labels,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "'''\n",
    "#Evaluate the model\n",
    "predictions = model.predict(test_inputs.input_ids)\n",
    "predicted_labels = eval_predictions = tf.argmax(tf.nn.softmax(predictions, axis=1), axis=1).numpy()\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1d4107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 122s 1s/step\n"
     ]
    }
   ],
   "source": [
    "#Encode predict input\n",
    "predict_inputs = tokenizer(x1.tolist(), padding=True, truncation=True, return_tensors='tf')\n",
    "# Make predictions\n",
    "predictions = model.predict(predict_inputs.input_ids)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850717c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93eca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test_df['id'], 'target': predicted_labels})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
